{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Analytics\n",
    "\n",
    "## Assignment 3\n",
    "\n",
    "**Instructor:** Dr. Marco D'Ambros  \n",
    "**TAs:** Carmen Armenti, Mattia Giannaccari\n",
    "\n",
    "**Contacts:** marco.dambros@usi.ch, carmen.armenti@usi.ch, mattia.giannaccari@usi.ch\n",
    "\n",
    "**Due Date:** May 16, 2025 @ 23:55\n",
    "\n",
    "---\n",
    "The goal of this assignment is to use **Spark (PySpark)** and **Polars** in Jupyter notebooks.  \n",
    "The files `trip_data.csv`, `trip_fare.csv`, and `nyc_boroughs.geojson` are available in the provided folder: [Assignment3-data](https://usi365-my.sharepoint.com/:f:/g/personal/armenc_usi_ch/Ejp7sb8QAMROoWe0XUDcAkMBoqUFk-w2Vgroup025NhAww?e=2I7SMC).\n",
    "\n",
    "You may clean the data as needed; however, please note that specific data cleaning steps will be required in **Exercise 5**. If you choose to clean the data before Exercise 5, make sure to retain the **original dataset** for use with the Polars exercises.\n",
    "\n",
    "- Use **Spark** to solve **Exercises 1–4**\n",
    "- Use **Polars** to solve **Exercises 5–8**\n",
    "\n",
    "You are encouraged to use [Spark window functions](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-window.html) whenever appropriate.\n",
    "\n",
    "Please name your notebook file as `SurnameName_Assignment3.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Join the `trip_data` and `trip_fare` dataframes into one and consider only data on 2013-01-01. Please specify the number of rows obtained after joining the 2 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DateType\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = SparkSession.builder.getOrCreate()\n",
    "session.conf.set(\"spark.sql.shuffle.partitions\", 400)\n",
    "session.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df = session.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"./datasets/trip_data.csv\")\n",
    "trip_fare_df = session.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"./datasets/trip_fare.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df.select([col(c).alias(c.strip()) for c in trip_data_df.columns]) \n",
    "trip_fare_df = trip_fare_df.select([col(c).alias(c.strip()) for c in trip_fare_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df = trip_data_df.select(\n",
    "    \"*\",\n",
    "    col(\"pickup_datetime\").cast(DateType()).alias(\"pickup_date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = trip_data_df.join(trip_fare_df, on=[\"medallion\", \"hack_license\", \"pickup_datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = joined_df.filter(joined_df.pickup_date == \"2013-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records for 2013-01-01: \", filtered_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print schema\n",
    "filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Provide a graphical representation to compare the average fare amount for trips _within_ and _across_ all the boroughs. You may want to have a look at: https://docs.bokeh.org/en/latest/docs/user_guide/topics/categorical.html#categorical-heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_df = gpd.read_file('./datasets/nyc-boroughs.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_pandas = filtered_df.toPandas()\n",
    "pickup_gdf = gpd.GeoDataFrame(\n",
    "    filtered_df_pandas,\n",
    "    geometry=gpd.points_from_xy(filtered_df_pandas['pickup_longitude'], filtered_df_pandas['pickup_latitude']),\n",
    "    crs=nyc_df.crs\n",
    ")\n",
    "\n",
    "dropoff_gdf = gpd.GeoDataFrame(\n",
    "    filtered_df_pandas,\n",
    "    geometry=gpd.points_from_xy(filtered_df_pandas['dropoff_longitude'], filtered_df_pandas['dropoff_latitude']),\n",
    "    crs=nyc_df.crs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial join to get borough names\n",
    "pickup_boroughs = gpd.sjoin(pickup_gdf, nyc_df, how=\"left\", predicate=\"within\")\n",
    "dropoff_boroughs = gpd.sjoin(dropoff_gdf, nyc_df, how=\"left\", predicate=\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to original DataFrame\n",
    "filtered_df_pandas[\"pickup_borough\"] = pickup_boroughs[\"borough\"].values\n",
    "filtered_df_pandas[\"dropoff_borough\"] = dropoff_boroughs[\"borough\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Back to Spark for aggregation\n",
    "spark_df = session.createDataFrame(filtered_df_pandas)\n",
    "\n",
    "trip_group_df = spark_df \\\n",
    "    .groupBy(['pickup_borough', 'dropoff_borough']) \\\n",
    "    .avg('fare_amount') \\\n",
    "    .withColumnRenamed('avg(fare_amount)', 'avg_fare')\n",
    "\n",
    "unique_borough = nyc_df['borough'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import BasicTicker, PrintfTickFormatter\n",
    "from bokeh.plotting import figure, show\n",
    "from pyspark.sql.functions import max as spark_max, min as spark_min\n",
    "from bokeh.transform import linear_cmap\n",
    "\n",
    "min = trip_group_df.agg(spark_min('avg_fare')).collect()[0][0]\n",
    "max = trip_group_df.agg(spark_max('avg_fare')).collect()[0][0]\n",
    "\n",
    "colors = [\"#03045e\", \"#023e8a\", \"#0077b6\", \"#0096c7\", \"#00b4d8\", \"#48cae4\", \"#90e0ef\", \"#ade8f4\", \"#caf0f8\"]\n",
    "\n",
    "TOOLS = \"hover\"\n",
    "TOOLTIPS = [\n",
    "    ('Pickup Borough', '@unique_borough'),\n",
    "    ('Dropoff Borough', '@unique_borough'),\n",
    "    ('Average Fare Amount', '@avg_fare{0.2f}')\n",
    "]\n",
    "\n",
    "p = figure(title=\"Average Fare Amount for Pickup and Dropoff Boroughs\",\n",
    "           x_range=unique_borough, y_range=unique_borough,\n",
    "           x_axis_location=\"above\", width=900, height=400,\n",
    "           tools=TOOLS, toolbar_location='below', tooltips=TOOLTIPS)\n",
    "\n",
    "p.grid.grid_line_color = None\n",
    "p.axis.axis_line_color = None\n",
    "p.axis.major_tick_line_color = None\n",
    "p.axis.major_label_text_font_size = \"7px\"\n",
    "p.axis.major_label_standoff = 0\n",
    "\n",
    "r = p.rect(x=\"pickup_borough\", y=\"dropoff_borough\", width=1, height=1, source=trip_group_df.toPandas(),\n",
    "           fill_color=linear_cmap(\"avg_fare\", colors[::-1], low=min, high=max),\n",
    "           line_color=None)\n",
    "\n",
    "p.add_layout(r.construct_color_bar(\n",
    "    major_label_text_font_size=\"7px\",\n",
    "    ticker=BasicTicker(desired_num_ticks=len(colors)),\n",
    "    formatter=PrintfTickFormatter(format=\"%d%%\"),\n",
    "    label_standoff=6,\n",
    "    border_line_color=None,\n",
    "    padding=5,\n",
    "), 'right')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Consider only Manhattan, Bronx and Brooklyn boroughs. Then create a dataframe that shows the total number of trips *within* the same borough and *across* all the other boroughs mentioned before (Manhattan, Bronx, and Brooklyn) where the passengers are more or equal than 3.\n",
    "\n",
    "For example, for Manhattan borough you should consider the total number of the following trips:\n",
    "- Manhattan → Manhattan\n",
    "- Manhattan → Bronx\n",
    "- Manhattan → Brooklyn\n",
    "\n",
    "You should then do the same for Bronx and Brooklyn boroughs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "boroughs = [\"Manhattan\", \"Bronx\", \"Brooklyn\"]\n",
    "\n",
    "filtered_df = spark_df.filter(\n",
    "    (col(\"pickup_borough\").isin(boroughs)) &\n",
    "    (col(\"dropoff_borough\").isin(boroughs)) &\n",
    "    (col(\"passenger_count\") >= 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define trip type: 'within' or 'across'\n",
    "labeled_df = filtered_df.withColumn(\n",
    "    \"trip_type\",\n",
    "    when(col(\"pickup_borough\") == col(\"dropoff_borough\"), \"within\")\n",
    "    .otherwise(\"across\")\n",
    ")\n",
    "\n",
    "# Step 3: Group by trip_type and count\n",
    "result_df = labeled_df.groupBy(\"trip_type\").count()\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Create a dataframe where each row represents a driver, and there is one column per borough.\n",
    "For each driver-borough, the dataframe provides the maximum number of consecutive trips\n",
    "for the given driver, within the given borough. Please consider only trips which were payed by card. \n",
    "\n",
    "For example, if for driver A we have (sorted by time):\n",
    "- Trip 1: Bronx → Bronx\n",
    "- Trip 2: Bronx \\→ Bronx\n",
    "- Trip 3: Bronx → Manhattan\n",
    "- Trip 4: Manhattan → Bronx.\n",
    "    \n",
    "The maximum number of consecutive trips for Bronx is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_trips = spark_df.filter(\n",
    "    (col(\"payment_type\") == \"CRD\") &\n",
    "    (col(\"pickup_borough\") == col(\"dropoff_borough\"))\n",
    ").orderBy(\"hack_license\", \"pickup_datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Please work on the merged dataset of trips and fares and perform the following data cleaning tasks:\n",
    "\n",
    "1. Remove trips with invalid locations (i.e. not in New York City);\n",
    "3. Remove trips with invalid amounts:\n",
    "    - Total amount must be greater than zero;\n",
    "    - Total amount must correspond to the sum of all the other amounts.\n",
    "5. Remove trips with invalid time:\n",
    "    - Pick-up before drop-off;\n",
    "    - Valid duration.\n",
    "\n",
    "After each data cleaning task, report how many rows where removed. Finally report:\n",
    "- Are there **duplicate trips**?\n",
    "- How many trips remain after cleaning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df = pl.read_csv(\n",
    "    \"./datasets/trip_data.csv\",\n",
    "    infer_schema_length=100,\n",
    "    has_header=True,\n",
    "    encoding=\"us-ascii\"\n",
    ")\n",
    "trip_fare_df = pl.read_csv(\n",
    "    \"./datasets/trip_fare.csv\",\n",
    "    infer_schema_length=100,\n",
    "    has_header=True,\n",
    "    schema_overrides={\" tip_amount\": pl.Float64},\n",
    "    encoding=\"us-ascii\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>medallion</th><th> hack_license</th><th> vendor_id</th><th> pickup_datetime</th><th> payment_type</th><th> fare_amount</th><th> surcharge</th><th> mta_tax</th><th> tip_amount</th><th> tolls_amount</th><th> total_amount</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;89D227B655E5C82AECF13C3F540D4C…</td><td>&quot;BA96DE419E711691B9445D6A6307C1…</td><td>&quot;CMT&quot;</td><td>&quot;2013-01-01 15:11:48&quot;</td><td>&quot;CSH&quot;</td><td>6.5</td><td>0.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>7.0</td></tr><tr><td>&quot;0BD7C8F5BA12B88E0B67BED28BEA73…</td><td>&quot;9FD8F69F0804BDB5549F40E9DA1BE4…</td><td>&quot;CMT&quot;</td><td>&quot;2013-01-06 00:18:35&quot;</td><td>&quot;CSH&quot;</td><td>6.0</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>7.0</td></tr><tr><td>&quot;0BD7C8F5BA12B88E0B67BED28BEA73…</td><td>&quot;9FD8F69F0804BDB5549F40E9DA1BE4…</td><td>&quot;CMT&quot;</td><td>&quot;2013-01-05 18:49:41&quot;</td><td>&quot;CSH&quot;</td><td>5.5</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>7.0</td></tr><tr><td>&quot;DFD2202EE08F7A8DC9A57B02ACB81F…</td><td>&quot;51EE87E3205C985EF8431D850C7863…</td><td>&quot;CMT&quot;</td><td>&quot;2013-01-07 23:54:15&quot;</td><td>&quot;CSH&quot;</td><td>5.0</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>6.0</td></tr><tr><td>&quot;DFD2202EE08F7A8DC9A57B02ACB81F…</td><td>&quot;51EE87E3205C985EF8431D850C7863…</td><td>&quot;CMT&quot;</td><td>&quot;2013-01-07 23:25:03&quot;</td><td>&quot;CSH&quot;</td><td>9.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>10.5</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 11)\n",
       "┌─────────────────────────────────┬─────────────────────────────────┬────────────┬─────────────────────┬───┬──────────┬─────────────┬───────────────┬───────────────┐\n",
       "│ medallion                       ┆  hack_license                   ┆  vendor_id ┆  pickup_datetime    ┆ … ┆  mta_tax ┆  tip_amount ┆  tolls_amount ┆  total_amount │\n",
       "│ ---                             ┆ ---                             ┆ ---        ┆ ---                 ┆   ┆ ---      ┆ ---         ┆ ---           ┆ ---           │\n",
       "│ str                             ┆ str                             ┆ str        ┆ str                 ┆   ┆ f64      ┆ f64         ┆ f64           ┆ f64           │\n",
       "╞═════════════════════════════════╪═════════════════════════════════╪════════════╪═════════════════════╪═══╪══════════╪═════════════╪═══════════════╪═══════════════╡\n",
       "│ 89D227B655E5C82AECF13C3F540D4C… ┆ BA96DE419E711691B9445D6A6307C1… ┆ CMT        ┆ 2013-01-01 15:11:48 ┆ … ┆ 0.5      ┆ 0.0         ┆ 0.0           ┆ 7.0           │\n",
       "│ 0BD7C8F5BA12B88E0B67BED28BEA73… ┆ 9FD8F69F0804BDB5549F40E9DA1BE4… ┆ CMT        ┆ 2013-01-06 00:18:35 ┆ … ┆ 0.5      ┆ 0.0         ┆ 0.0           ┆ 7.0           │\n",
       "│ 0BD7C8F5BA12B88E0B67BED28BEA73… ┆ 9FD8F69F0804BDB5549F40E9DA1BE4… ┆ CMT        ┆ 2013-01-05 18:49:41 ┆ … ┆ 0.5      ┆ 0.0         ┆ 0.0           ┆ 7.0           │\n",
       "│ DFD2202EE08F7A8DC9A57B02ACB81F… ┆ 51EE87E3205C985EF8431D850C7863… ┆ CMT        ┆ 2013-01-07 23:54:15 ┆ … ┆ 0.5      ┆ 0.0         ┆ 0.0           ┆ 6.0           │\n",
       "│ DFD2202EE08F7A8DC9A57B02ACB81F… ┆ 51EE87E3205C985EF8431D850C7863… ┆ CMT        ┆ 2013-01-07 23:25:03 ┆ … ┆ 0.5      ┆ 0.0         ┆ 0.0           ┆ 10.5          │\n",
       "└─────────────────────────────────┴─────────────────────────────────┴────────────┴─────────────────────┴───┴──────────┴─────────────┴───────────────┴───────────────┘"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_fare_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip data columns:  ['medallion', 'hack_license', 'vendor_id', 'rate_code', 'store_and_fwd_flag', 'pickup_datetime', 'dropoff_datetime', 'passenger_count', 'trip_time_in_secs', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "Trip fare columns:  ['medallion', ' hack_license', ' vendor_id', ' pickup_datetime', ' payment_type', ' fare_amount', ' surcharge', ' mta_tax', ' tip_amount', ' tolls_amount', ' total_amount']\n"
     ]
    }
   ],
   "source": [
    "print(\"Trip data columns: \", trip_data_df.columns)\n",
    "print(\"Trip fare columns: \", trip_fare_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df = trip_data_df.rename({name: name.strip() for name in trip_data_df.columns})\n",
    "trip_fare_df = trip_fare_df.rename({name: name.strip() for name in trip_fare_df.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = trip_data_df.join(trip_fare_df, on=[\"medallion\", \"hack_license\", \"pickup_datetime\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def get_min_max_coordinates(\n",
    "    geojson: dict[str, Any],\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Get the min/max coordinates from a geojson file.\n",
    "    \"\"\"\n",
    "    # get all the coordinates from the geojson file\n",
    "    coordinates: list[tuple[float, float]] = []\n",
    "    for feature in geojson[\"features\"]:\n",
    "        coordinates.extend(feature[\"geometry\"][\"coordinates\"][0])\n",
    "\n",
    "    # get the min/max coordinates\n",
    "    min_lon = min([point[0] for point in coordinates])\n",
    "    max_lon = max([point[0] for point in coordinates])\n",
    "    min_lat = min([point[1] for point in coordinates])\n",
    "    max_lat = max([point[1] for point in coordinates])\n",
    "\n",
    "    return min_lon, max_lon, min_lat, max_lat\n",
    "\n",
    "nyc_boroughs_geojson_path = Path(\"./datasets/nyc-boroughs.geojson\")\n",
    "\n",
    "with nyc_boroughs_geojson_path.open(\"r\") as f:\n",
    "    json_data = f.read()\n",
    "    nyc_boroughs_geo_data: dict[str, Any] = json.loads(json_data)\n",
    "# get the min/max coordinates\n",
    "min_lon, max_lon, min_lat, max_lat = get_min_max_coordinates(nyc_boroughs_geo_data)\n",
    "\n",
    "print(f\"Min lon: {min_lon}, Max lon: {max_lon}\")\n",
    "print(f\"Min lat: {min_lat}, Max lat: {max_lat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ny_df = joined_df.filter(\n",
    "    pl.col(\"pickup_longitude\") > min_lon,\n",
    "    pl.col(\"pickup_longitude\") < max_lon,\n",
    "    pl.col(\"pickup_latitude\") > min_lat,\n",
    "    pl.col(\"pickup_latitude\") < max_lat,\n",
    "    pl.col(\"dropoff_longitude\") > min_lon,\n",
    "    pl.col(\"dropoff_longitude\") < max_lon,\n",
    "    pl.col(\"dropoff_latitude\") > min_lat,\n",
    "    pl.col(\"dropoff_latitude\") < max_lat,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ny_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ny_df = filtered_ny_df.filter(\n",
    "    (pl.col(\"total_amount\") >= 0) & \n",
    "    (pl.col(\"total_amount\") == (pl.col(\"fare_amount\") + pl.col(\"surcharge\") + pl.col(\"mta_tax\")+ pl.col(\"tip_amount\")+ pl.col(\"tolls_amount\"))) &\n",
    "    (pl.col(\"pickup_datetime\") < pl.col(\"dropoff_datetime\")) &\n",
    "    (pl.col('dropoff_datetime') - pl.col('pickup_datetime')).dt.total_seconds() == pl.col('trip_time_in_secs')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records before filtering: \", joined_df.shape[0])\n",
    "print(\"Number of records after filtering: \", filtered_ny_df.shape[0])\n",
    "print(\"Deleted records: \", joined_df.shape[0] - filtered_ny_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "Compute the **total revenue** (total_amount) grouped by:\n",
    "- Pick-up hour of the day (0–23)\n",
    "- Passenger count (group >=6 into “6+”)\n",
    "\n",
    "Create a heatmap where:\n",
    "- X-axis = hour\n",
    "- Y-axis = passenger count group\n",
    "- Cell value = average revenue per trip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "Define an \"anomalous trip\" as one that satisfies at least two of the following:\n",
    "- Fare per mile is above the 95th percentile\n",
    "- Tip amount > 100% of fare\n",
    "- trip_time_in_secs is less than 60 seconds but distance is more than 1 mile\n",
    "\n",
    "Create a dataframe of anomalous trips and:\n",
    "- Report how many such trips exist\n",
    "- Create a scatterplot to visualize the anomaly metrics\n",
    "- Describe the visualization identifying groups and outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "For each driver (hack_license), calculate the **total profit per hour worked**, where:\n",
    "> profit = 0.7 * (fare_amount + tip_amount) when the trip starts between 7:01 AM and 7:00 PM\\\n",
    "> profit = 0.8 * (fare_amount + tip_amount) when the trip starts between 7:01PM and 7:00 AM\n",
    "\n",
    "Estimate \"hours worked\" by summing trip_time_in_secs.\n",
    "\n",
    "Plot a line chart showing the distribution of average profit per hour **for the top 10% drivers** in terms of total trips.\n",
    "\n",
    "Which time of day offers **best earning efficiency**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "va3",
   "language": "python",
   "name": "va3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
